{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbIF4OLVsPxb"
   },
   "source": [
    "# Homework #1 (Due 09/18/2019, 11:59pm)\n",
    "## Maximum Likelihood Learning and Bayesian Inference\n",
    "\n",
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGeygi3msPxg"
   },
   "source": [
    "**Name:** Rylan Schaeffer\n",
    "\n",
    "**Students collaborators:** Theo Guenais, Dimitrios Vamvourellis, Dmitry Vukolov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXk9-5ScsPxj"
   },
   "source": [
    "### Instructions:\n",
    "\n",
    "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
    "\n",
    "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. Portions of your submission that contains syntactic or run-time errors will not be graded.\n",
    "\n",
    "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2g0YN8iusPxm"
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIJJkRO4sgAZ"
   },
   "source": [
    "This cell is for defining LaTeX operators. Do not delete!!!\n",
    "$\\newcommand{\\argmin}[1]{\\underset{{#1}}{\\mathrm{argmin}}}$\n",
    "$\\newcommand{\\argmax}[1]{\\underset{{#1}}{\\mathrm{argmax}}}$\n",
    "$\\DeclareMathOperator{\\defeq}{\\stackrel{def}{=}}$\n",
    "$\\DeclareMathOperator{\\Tr}{Tr}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVTlYQrwtIiy"
   },
   "outputs": [],
   "source": [
    "# # connect to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# # Change to correct homework directory\n",
    "# import os\n",
    "# target_dir = '/content/gdrive/My Drive/Colab Notebooks/am207/hw1'\n",
    "# if os.getcwd() != target_dir:\n",
    "#   os.chdir(target_dir)\n",
    "# assert os.getcwd() == target_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sngxBX1ysPxv"
   },
   "source": [
    "## Problem Description\n",
    "In the competitive rubber chicken retail market, the success of a company is built on satisfying the exacting standards of a consumer base with refined and discriminating taste. In particular, customer product reviews are all important. But how should we judge the quality of a product based on customer reviews?\n",
    "\n",
    "On Amazon, the first customer review statistic displayed for a product is the ***average rating***. The following are the main product pages for two competing rubber chicken products, manufactured by Lotus World and Toysmith respectively:\n",
    "\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus1.png) |  ![alt](toysmith1.png)\n",
    "\n",
    "Clicking on the 'customer review' link on the product pages takes us to a detailed break-down of the reviews. In particular, we can now see the number of times a product is rated a given rating (between 1 and 5 stars).\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus2.png) |  ![alt](toysmith2.png)\n",
    "\n",
    "\n",
    "In the following, we will ask you to build statistical models to compare these two products using the observed rating. Larger versions of the images are available in the data set accompanying this notebook.\n",
    "\n",
    "\n",
    "\n",
    "## Part I: A Maximum Likelihood Model\n",
    "1. **(Model Building)** Suppose that for each product, we can model the probability of the value each new rating as the following vector:\n",
    "$$\n",
    "\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5]\n",
    "$$\n",
    "  where $\\theta_i$ is the probability that a given customer will give the product $i$ number of stars. That is, each new rating (a value between 1 and 5) has a categorical distribution $Cat(\\theta)$. Represent the observed ratings of an Amazon product as a vector $R = [r_1, r_2, r_3, r_4, r_5]$ where, for example, $r_4$ is the number of $4$-star reviews out of a total of $N$ ratings. Write down the likelihood of $R$. That is, what is $p(R| \\theta)$?\n",
    "\n",
    "  **Note:** The observed ratings for each product should be read off the image files included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-rnol2ftndX"
   },
   "source": [
    "The ratings follow a Multinomial distribution:\n",
    "\n",
    "\\begin{align}\n",
    "P(R|\\theta) = \\frac{(\\sum_{i=1}^5 r_i)!}{\\prod_{i=1}^5 r_i!} \\prod_{i=1}^5 \\theta_i^{r_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sxx_3XYzsPx2"
   },
   "source": [
    "2. **(Model Fitting)** Find the maximum likelihood estimator of $\\theta$ for the Lotus World model; find the MLE of $\\theta$ for the Toysmith model. You need to make a reasonably mathematical argument for why your estimate actually maximizes the likelihood (i.e. recall the criteria for a point to be a global optima of a function).\n",
    "\n",
    "  *Note:* I recommend deriving the MLE using the general expression of the likelihood. That is, derive the posterior using the variable $R$, then afterwards plug in your specific values of $R$ for each product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKGcfArqv9hW"
   },
   "source": [
    "I first derive the maximum likelihood estimator of $\\theta$ for a generic categorical distribution and then use the result to compute the MLE for both the Lotus World model and the Toysmith model. In this case, we have a constrained optimization problem: find $\\theta$ that maximizes the log likelihood subject to the constraint that $\\sum_{i=1}^k \\theta_i = 1$. We define the Lagrangian accordingly, take the derivative, set equal to 0 and solve:\n",
    "\n",
    "\\begin{align}\n",
    "L[\\theta] &\\defeq - \\log P(R|\\theta) + \\lambda (\\sum_{i=1}^k \\theta_i)\\\\\n",
    "\\frac{\\partial L[\\theta]}{\\partial \\theta_j} = 0\n",
    "&= - \\frac{\\partial}{\\partial \\theta_j} \\log \\prod_{i=1}^k \\theta_i^{r_i} + \\frac{\\partial}{\\partial \\theta_j} \\lambda (\\sum_{i=1}^k \\theta_i)\\\\\n",
    "&= - \\frac{r_j}{\\theta_j} + \\lambda\\\\\n",
    "\\theta_j &= \\frac{r_j}{\\lambda}\n",
    "\\end{align}\n",
    "\n",
    "Using the constraint $\\sum_{i=1}^k \\theta_i = 1$, we see that $\\lambda = \\sum_{i=1}^k r_i$. Plugging in $\\lambda$, we discover the maximum likelihood estimator:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_j^{MLE} &= \\frac{r_j}{\\sum_{i=1}^k r_i}\n",
    "\\end{align}\n",
    "\n",
    "We can demonstrate that this is a maximum by showing that elements of the Hessian of the log likelihood are all negative (since $r_ij \\geq 0$):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log P(\\theta|R)}{\\partial \\theta_j \\partial \\theta_i}\n",
    "&= \\frac{\\partial }{\\partial \\theta_i} [\\frac{r_j}{\\theta_j}]\\\\\n",
    "&= \\begin{cases}\n",
    "-\\frac{r_j}{\\theta_j^2} & i ==j\\\\\n",
    "0 & i \\neq j\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "We can then compute the MLE for the Lotus World model, $\\theta_{L}^{MLE}$, and the MLE for the Toysmith model, $\\theta_{T}^{MLE}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KsmyJBDIsPx4",
    "outputId": "ef2569a1-85e5-4640-d57b-384f875c1960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus MLE:  [0.06 0.04 0.06 0.17 0.67]\n",
      "Toysmith MLE:  [0.14 0.08 0.07 0.11 0.6 ]\n"
     ]
    }
   ],
   "source": [
    "theta_mle_lotus = np.array([.06, .04, .06, .17, .67])\n",
    "print('Lotus MLE: ', theta_mle_lotus)\n",
    "theta_mle_toysmith = np.array([.14, .08, .07, .11, .60])\n",
    "print('Toysmith MLE: ', theta_mle_toysmith)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRPwDyeisPx8"
   },
   "source": [
    "3. **(Model Interpretation)** Based on your MLE of $\\theta$'s for both models, do you feel confident deciding if one product is superior to another? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iDVYo51t0Xs"
   },
   "source": [
    "I would lean towards the Lotus being superior, BUT I would not feel extremely confident due to the smaller sample size. 410 people have reviewed the Toysmith, whereas only 162 people have reviewed the Lotus. One indicator that the Toysmith is worse is that a 14% of reviewers rated the product 1 out of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEymm8Q9sPyH"
   },
   "source": [
    "## Part II: A Bayesian Model\n",
    "\n",
    "1. **(Model Building)** Suppose you are told that customer opinions are very polarized in the retail world of rubber chickens, that is, most reviews will be 5 stars or 1 stars (with little middle ground). What would be an appropriate $\\alpha$ for the Dirichlet prior on $\\theta$? Recall that the Dirichlet pdf is given by:\n",
    "$$\n",
    "p_{\\Theta}(\\theta) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}, \\quad B(\\alpha) = \\frac{\\prod_{i=1}^k\\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum_{i=1}^k\\alpha_i\\right)},\n",
    "$$\n",
    "where $\\theta_i \\in (0, 1)$ and $\\sum_{i=1}^k \\theta_i = 1$, $\\alpha_i > 0 $ for $i = 1, \\ldots, k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lS9-wMOAIS7I"
   },
   "source": [
    "I would expect $\\alpha_1$ and $\\alpha_5$ to be relatively large compared to $\\alpha_2, \\alpha_3, \\alpha_4$. Giving precise values would be difficult since I don't know enough regarding the ratio of the $\\alpha_i$ e.g., I couldn't say whether $\\alpha = \\begin{bmatrix} 10\\\\ 1\\\\ 1\\\\1\\\\ 10\\end{bmatrix}$ or $\\alpha = \\begin{bmatrix} 100\\\\ 1\\\\ 1\\\\1\\\\ 100\\end{bmatrix}$ would be more appropriate. I'll ARBITRARILY choose the second. Note that this choice of prior also implies that a 1-star review is as likely as a 5 star review, which may not be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lX7ec3zZsPyM"
   },
   "source": [
    "2. **(Inference)** Analytically derive the posterior distribution (using the likelihoods you derived in Part I) for each product.\n",
    "\n",
    "  *Note:* I recommend deriving the posterior using the general expression of a Dirichelet pdf. That is, derive the posterior using the variable $\\alpha$, then afterwards plug in your specific values of $\\alpha$ when you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LbPEpvLLt37"
   },
   "source": [
    "We start with Bayes rule:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\theta|R, \\alpha) \n",
    "&= \\frac{P(R|\\theta, \\alpha)P(\\theta | \\alpha)}{P(R | \\alpha)}\\\\\n",
    "&\\propto P(R|\\theta, \\alpha)P(\\theta | \\alpha)\\\\\n",
    "&\\propto \\prod_{i=1}^k \\theta_i^{r_i} \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}\\\\\n",
    "&\\propto \\prod_{i=1}^k \\theta_i^{r_i + \\alpha_i -1}\n",
    "\\end{align}\n",
    "\n",
    "From this, we can infer that the posterior $P(\\theta|R, \\alpha) \\sim \\text{Dirichlet}(\\begin{bmatrix} r_1 + \\alpha_1 \\\\ r_2 + \\alpha_2 \\\\ \\vdots \\\\ r_k + \\alpha_k\\end{bmatrix})$. Consequently, the posteriors for both models are as follows:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\theta_{Lotus}|R, \\alpha) &\\sim \\text{Dirichlet}(\\begin{bmatrix} 10 + \\alpha_1 \\\\ 6 + \\alpha_2 \\\\ 10 + \\alpha_3 \\\\ 28 + \\alpha_4 \\\\ 109 + \\alpha_5\\end{bmatrix})\\\\\n",
    "P(\\theta_{Toysmith}|R, \\alpha) &\\sim \\text{Dirichlet}(\\begin{bmatrix} 57 + \\alpha_1 \\\\ 33 + \\alpha_2 \\\\ 29 + \\alpha_3 \\\\ 45 + \\alpha_4 \\\\ 246 + \\alpha_5\\end{bmatrix})\n",
    "\\end{align}\n",
    "\n",
    "Assuming $\\alpha = \\begin{bmatrix} 100\\\\ 1\\\\ 1\\\\1\\\\ 100\\end{bmatrix}$, we can compute the parameters defining the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "LjefMlSvPF7X",
    "outputId": "b8fcb0ef-6b7c-4025-b822-6fa4b9e3aa94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus P(theta|R,alpha):  [110.   7.  11.  29. 209.]\n",
      "Toysmith P(theta|R, alpha):  [157.  34.  30.  46. 346.]\n"
     ]
    }
   ],
   "source": [
    "alpha = np.array([100, 1, 1, 1, 100])\n",
    "r_lotus = np.round(162*np.array([.06, .04, .06, .17, .67]))\n",
    "r_toysmith = np.round(410*np.array([.14, .08, .07, .11, .60]))\n",
    "theta_post_lotus = r_lotus + alpha\n",
    "print('Lotus P(theta|R,alpha): ', theta_post_lotus)\n",
    "theta_post_toysmith = r_toysmith + alpha\n",
    "print('Toysmith P(theta|R, alpha): ', theta_post_toysmith)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WDCaAYIsPyU"
   },
   "source": [
    "3. **(The Maximum A Posterior Estimate)** Analytically or empirically compute the MAP estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MLE? Just for this problem, compute the MAP estimate of $\\theta$ for each product using a Dirichelet prior with hyperparameters $\\alpha = [1, 1, 1, 1, 1]$. Make a conjecture about the effect of the prior on the difference between the MAP estimates and the MLE's of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xk1U9Tb9Sq38"
   },
   "source": [
    "I first derive the MAP estimator of $\\theta$ for a generic Dirichlet distribution and then use the result to compute the MAP for both the Lotus World model and the Toysmith model. In this case, we have a constrained optimization problem: find $\\theta$ that maximizes the posterior subject to the constraint that $\\sum_{i=1}^k \\theta_i = 1$. We define the Lagrangian accordingly, take the derivative, set equal to 0 and solve:\n",
    "\n",
    "\\begin{align}\n",
    "L[\\theta] &\\defeq - \\log P(\\theta | R, \\alpha) + \\lambda (\\sum_{i=1}^k \\theta_i)\\\\\n",
    "\\frac{\\partial L[\\theta]}{\\partial \\theta_j} = 0\n",
    "&= -\\frac{\\partial}{\\partial \\theta_j} \\log \\frac{1}{B(R + \\alpha)} \\prod_{i=1}^k \\theta_i^{r_i + \\alpha_i - 1} + \\frac{\\partial}{\\partial \\theta_j}\\lambda (\\sum_{i=1}^k \\theta_i)\\\\\n",
    "\\lambda &= \\frac{r_j + \\alpha_j - 1}{\\theta_j}\n",
    "\\end{align}\n",
    "\n",
    "Using the constraint $\\sum_{i=1}^k \\theta_i = 1$, we see that $\\lambda = \\sum_{i=1}^k r_i + \\alpha_i - 1$. Plugging in $\\lambda$, we discover the MAP estimator:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_j^{MAP} &= \\frac{r_j + \\alpha_j - 1}{\\sum_{i=1}^k (r_i + \\alpha_i - 1)}\n",
    "\\end{align}\n",
    "\n",
    "We can then compute the MAP for the Lotus World model, $\\theta_{L}^{MAP}$, and the MAP for the Toysmith model, $\\theta_{T}^{MAP}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mF5zgmWWZmjB",
    "outputId": "c1cd76fb-7c82-48f4-a226-ae7808858135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus MAP Estimate:  [0.30193906 0.0166205  0.02770083 0.07756233 0.57617729]\n",
      "Toysmith MAP Estimate:  [0.25657895 0.05427632 0.04769737 0.07401316 0.56743421]\n"
     ]
    }
   ],
   "source": [
    "theta_map_lotus = r_lotus + alpha - 1\n",
    "theta_map_lotus = theta_map_lotus / np.sum(theta_map_lotus)\n",
    "print('Lotus MAP Estimate: ', theta_map_lotus)\n",
    "theta_map_toysmith = r_toysmith + alpha - 1\n",
    "theta_map_toysmith = theta_map_toysmith / np.sum(theta_map_toysmith)\n",
    "print('Toysmith MAP Estimate: ', theta_map_toysmith)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LyAJ6QlsPyb"
   },
   "source": [
    "4. **(The Posterior Mean Estimate)** Analytically or empirically compute the posterior mean estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MAP estimates and the MLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iv0NNFBfadjI"
   },
   "source": [
    "The first moment of the Dirichlet distribution is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\theta_j] &= \\frac{r_j + \\alpha_j}{\\sum_{i=1}^k (r_i + \\alpha_i)}\n",
    "\\end{align}\n",
    "\n",
    "We can empirically compute these using the previously chosen $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KqboI001sPyd",
    "outputId": "16fce5fe-3044-40b1-b3de-3ea9188b3113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus Posterior Mean:  [0.30054645 0.01912568 0.03005464 0.07923497 0.57103825]\n",
      "Toysmith Posterior Mean:  [0.25611746 0.05546493 0.04893964 0.07504078 0.56443719]\n"
     ]
    }
   ],
   "source": [
    "theta_post_mean_lotus = r_lotus + alpha\n",
    "theta_post_mean_lotus = theta_post_mean_lotus / np.sum(theta_post_mean_lotus)\n",
    "print('Lotus Posterior Mean: ', theta_post_mean_lotus)\n",
    "theta_post_mean_toysmith = r_toysmith + alpha \n",
    "theta_post_mean_toysmith = theta_post_mean_toysmith / np.sum(theta_post_mean_toysmith)\n",
    "print('Toysmith Posterior Mean: ', theta_post_mean_toysmith)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrbkC0Z_sPyj"
   },
   "source": [
    "5. **(The Posterior Predictive Estimate)** Sample 1000 rating vectors from the posterior predictive for each product, using the $\\alpha$'s you chose in Problem 1. Use the average of the posterior predictive samples to estimate $\\theta$. How do these estimates compare with the MAP, MLE, posterior mean estimate of $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "2XQyL59dsPyk",
    "outputId": "09077266-aa0d-43b7-d6d0-ebc38b51cb31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus Posterior Predictive Mean:  [0.3012  0.01931 0.03003 0.08009 0.56937]\n",
      "Lotus MAP Estimate:  [0.30193906 0.0166205  0.02770083 0.07756233 0.57617729]\n",
      "Lotus Posterior Mean:  [0.30054645 0.01912568 0.03005464 0.07923497 0.57103825]\n",
      "Lotus MLE:  [0.06 0.04 0.06 0.17 0.67]\n",
      "\n",
      "Toysmith Posterior Predictive Mean:  [0.25849 0.05598 0.04925 0.07436 0.56192]\n",
      "Toysmith MAP Estimate:  [0.25657895 0.05427632 0.04769737 0.07401316 0.56743421]\n",
      "Toysmith Posterior Mean:  [0.25611746 0.05546493 0.04893964 0.07504078 0.56443719]\n",
      "Toysmith MLE:  [0.14 0.08 0.07 0.11 0.6 ]\n"
     ]
    }
   ],
   "source": [
    "def multinomial_posterior_predictive(theta):\n",
    "  x_samples = np.random.multinomial(n=100, pvals=theta)\n",
    "  theta_sample = x_samples / np.sum(x_samples)\n",
    "  return theta_sample\n",
    "\n",
    "\n",
    "lotus_post_pred_samples = np.random.dirichlet(alpha=theta_post_lotus, size=1000)\n",
    "lotus_post_pred_mean = np.mean(\n",
    "    np.apply_along_axis(\n",
    "        multinomial_posterior_predictive,\n",
    "        axis=1,\n",
    "        arr=lotus_post_pred_samples),\n",
    "    axis=0)\n",
    "print('Lotus Posterior Predictive Mean: ', lotus_post_pred_mean)\n",
    "print('Lotus MAP Estimate: ', theta_map_lotus)\n",
    "print('Lotus Posterior Mean: ', theta_post_mean_lotus)\n",
    "print('Lotus MLE: ', theta_mle_lotus)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "toysmith_post_pred_samples = np.random.dirichlet(alpha=theta_post_toysmith, size=1000)\n",
    "lotus_post_pred_mean = np.mean(\n",
    "    np.apply_along_axis(\n",
    "        multinomial_posterior_predictive,\n",
    "        axis=1,\n",
    "        arr=toysmith_post_pred_samples),\n",
    "    axis=0)\n",
    "print('Toysmith Posterior Predictive Mean: ', lotus_post_pred_mean)\n",
    "print('Toysmith MAP Estimate: ', theta_map_toysmith)\n",
    "print('Toysmith Posterior Mean: ', theta_post_mean_toysmith)\n",
    "print('Toysmith MLE: ', theta_mle_toysmith)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYbqB6Vue308"
   },
   "source": [
    "For both products, each of the four estimators differs slightly from the others. A few observations.\n",
    "\n",
    "1. The magnitude of the difference between the different estimators is SMALLER for the Lotus estimators than for the Toysmith estimators because the likelihood had more observations (approximately 4 times more), meaning the prior had comparatively less of an influence on the Toysmith posterior than the prior had on the Lotus posterior.\n",
    "2. The MLE differs the most from the posterior predictive mean, the MAP estimator and the posterior mean, which makes sense as the MLE is the only one of the four that does not include information from the prior.\n",
    "3. For both products, the Posterior Mean and the Posterior Predictive Mean very nearly agree. This makes sense as the first is an analytical expression for the quantity that the second determines via sampling.\n",
    "4. For both products, the MAP estimates are almost equal to the Posterior Means, but both differ slightly. This is likely due to the -1 in the formula for the MAP estimate that isn't included in the formula for the posterior mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G3I6S1ssPyp"
   },
   "source": [
    "6. **(Model Evaluation)** Compute the 95% credible interval of $\\theta$ for each product (*Hint: compute the 95% credible interval for each $\\theta_i$, $i=1, \\ldots, 5$*). For which product is the posterior mean and MAP estimate more reliable and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "Y-LpwEpVsPyr",
    "outputId": "e1f87a2a-b627-4b75-aafa-4be3e2fee5de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotus 95% Credible Interval:\n",
      " [[0.25466744 0.00774395 0.01513559 0.05381407 0.52004173]\n",
      " [0.3485079  0.03541056 0.04977125 0.10889812 0.62137004]]\n",
      "Lotus MAP Estimate:  [0.30193906 0.0166205  0.02770083 0.07756233 0.57617729]\n",
      "Lotus Posterior Mean:  [0.30054645 0.01912568 0.03005464 0.07923497 0.57103825]\n",
      "\n",
      "Toysmith 95% Credible Interval:\n",
      " [[0.22227944 0.03878721 0.03331062 0.05557806 0.52512524]\n",
      " [0.29135443 0.07482961 0.06727662 0.09714053 0.6033123 ]]\n",
      "Toysmith MAP Estimate:  [0.25657895 0.05427632 0.04769737 0.07401316 0.56743421]\n",
      "Toysmith Posterior Mean:  [0.25611746 0.05546493 0.04893964 0.07504078 0.56443719]\n"
     ]
    }
   ],
   "source": [
    "lotus_post_pred_samples = np.random.dirichlet(alpha=theta_post_lotus, size=500000)\n",
    "lotus_credible_interval = np.percentile(lotus_post_pred_samples,\n",
    "                                        [2.5, 97.5],\n",
    "                                        axis=0)\n",
    "print('Lotus 95% Credible Interval:\\n', lotus_credible_interval)\n",
    "print('Lotus MAP Estimate: ', theta_map_lotus)\n",
    "print('Lotus Posterior Mean: ', theta_post_mean_lotus)\n",
    "\n",
    "print()\n",
    "\n",
    "toysmith_post_pred_samples = np.random.dirichlet(alpha=theta_post_toysmith, size=500000)\n",
    "toysmith_credible_interval = np.percentile(toysmith_post_pred_samples,\n",
    "                                           [2.5, 97.5],\n",
    "                                           axis=0)\n",
    "print('Toysmith 95% Credible Interval:\\n', toysmith_credible_interval)\n",
    "print('Toysmith MAP Estimate: ', theta_map_toysmith)\n",
    "print('Toysmith Posterior Mean: ', theta_post_mean_toysmith)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhEOVYWflB_S"
   },
   "source": [
    "I find that for both products, the Toysmith model is BETTER than the Lotus model. For both products, the MAP and the posterior mean fall comfortably within the credible intervals. To quantify which is better centered, we can take the norm of the difference between the middle of the credible intervals and the two quantitites to see which is better; as shown below, the distance of the Toysmith MAP and Posterior Mean are closer to the center of their respective credible intervals than the Lotus MAP and Posterior Mean. We can also see that the Toysmith intervals are narrower, suggesting greater confidence in the parameter, although we know from HW0 that this can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "Nnj6MnCwkTHA",
    "outputId": "b54d046c-495b-4c5c-e173-00fe21f56bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between Lotus MAP and Center of Credible Interval:  0.009571256633588168\n",
      "Distance between Lotus Mean and Center of Credible Interval:  0.004178292819837969\n",
      "\n",
      "Distance between Toysmith MAP and Center of Credible Interval:  0.005389995588372615\n",
      "Distance between Toysmith Mean and Center of Credible Interval:  0.002431801549371792\n"
     ]
    }
   ],
   "source": [
    "print('Distance between Lotus MAP and Center of Credible Interval: ',\n",
    "      np.linalg.norm(np.mean(lotus_credible_interval, axis=0) - theta_map_lotus))\n",
    "print('Distance between Lotus Mean and Center of Credible Interval: ',\n",
    "      np.linalg.norm(np.mean(lotus_credible_interval, axis=0) - theta_post_mean_lotus))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Distance between Toysmith MAP and Center of Credible Interval: ',\n",
    "      np.linalg.norm(np.mean(toysmith_credible_interval, axis=0) - theta_map_toysmith))\n",
    "print('Distance between Toysmith Mean and Center of Credible Interval: ',\n",
    "      np.linalg.norm(np.mean(toysmith_credible_interval, axis=0) - theta_post_mean_toysmith))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jE9wvCOlsPyw"
   },
   "source": [
    "## Part III: Comparison\n",
    "1. **(Summarizing Customer Ratings)** Recall that on Amazon, the first customer review statistic displayed for a product is the average rating. Name at least one problem with ranking products based on the average customer rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3MqZHJbti2z"
   },
   "source": [
    "To name a couple:\n",
    "1. Average rating is overly sensitive to outliers. For instance, if most people rank a product 4 or 5, but a sizeable minority rank the product 1, then the average will be pulled down significantly.\n",
    "2. Average rating doesn't provide information regarding the variance. A distribution centered at 3.5 but sharply peaked will appear no different than a distribution centered at 3.5 but widely spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2FJ5atWsPy3"
   },
   "source": [
    "2. **(Comparison of Point Estimates)** Which point estimate (MAP, MLE, posterior mean or posterior predictive estimate) of $\\theta$, if any, would you feel choose to rank the two Amazon products? Why? \n",
    "\n",
    "  *Hint: think about which of these estimates are equivalent (if any). If they are not equivalent, what are the special properties of each estimate? What aspect of the data or the model is each estimate good at capturing?*\n",
    "  \n",
    "   **Note:** we're not looking for \"the correct answer\" here. We are looking for a sound decision based on a statistically correct interpretation of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfKLfQjmufoB"
   },
   "source": [
    "I personally found defining a reasonable prior to be highly problematic. As I noted above, there are many ways to interpret the claim that, \"Suppose you are told that [...] most reviews will be 5 stars or 1 stars (with little middle ground).\" Depending on your interpretation, the magnitude of $\\alpha$ and the elements of $\\alpha$ could be highly variable, and thus the conclusions of Bayesian methods (MAP, posterior mean, posterior predictive) would be variable e.g. if the magnitude of $\\alpha$ increases, then the likelihood will be less influential on the posterior. For instance, I could define $\\alpha = [0.00001, 0.00001, ...]$ and guarantee a MAP estimate as close to the MLE as I wanted, or I could define $\\alpha = [10^9, 10^9, ...]$ and guarantee that the MLE is effectively irrelevant.\n",
    "\n",
    "I would feel more comfortable if I had a quantitatively-created prior e.g. if someone averaged over previous ratings for other relevant products or if someone gave me a poll of product users' ratings. I could then use the Bayesian methods with confidence. But until then, I think the MLE is less susceptible to these subjective statistician preferences."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AM207_HW1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
