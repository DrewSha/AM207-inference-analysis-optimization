{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #15: Black-box Variational Inference\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats.multivariate_normal as mvn\n",
    "import autograd.scipy.stats.norm as norm\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam\n",
    "import numpy\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML\n",
    "from IPython.display import YouTubeVideo\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(N, y_var=1.):\n",
    "    '''generate training data with a gap, testing data uniformly sampled'''\n",
    "    #training x\n",
    "    x_train = np.hstack((np.linspace(-10, -5, N), np.linspace(5, 10, N)))\n",
    "    #function relating x and y\n",
    "#     f = lambda x:  0.01 * x**3\n",
    "    f = lambda x: 2 * x\n",
    "    #y is equal to f(x) plus gaussian noise\n",
    "    y_train = f(x_train) + np.random.normal(0, y_var**0.5, 2 * N)\n",
    "\n",
    "    ## generate testing data\n",
    "    #nubmer of testing points\n",
    "    N_test = 100\n",
    "    #testing x\n",
    "    x_test = np.linspace(-10, 10, N_test)\n",
    "    y_test = f(x_test) + np.random.normal(0, y_var**0.5, N_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def bayesian_polynomial_regression(x, y, x_test, y_test, prior_var, y_var, ax, S=100, poly_degree=10):\n",
    "    '''visualize the posterior predictive of a Bayesian polynomial regression model'''\n",
    "    poly = PolynomialFeatures(poly_degree)\n",
    "    #transform training x: add polynomial features\n",
    "    x_poly = poly.fit_transform(x.reshape((-1, 1)))\n",
    "    #transform x_test: add polynomial features\n",
    "    x_test_poly = poly.fit_transform(x_test.reshape((-1, 1)))\n",
    "    #Gaussian log pdf\n",
    "    gaussian_log_pdf = lambda mu, sigma_sq, x: -0.5 * (np.log(2 * np.pi * sigma_sq) + (x - mu)**2 / sigma_sq)\n",
    "\n",
    "    #reshape y into 2D array\n",
    "    y_matrix = y.reshape((-1, 1))\n",
    "\n",
    "    #define the covariance and precision matrices of the prior on the weights\n",
    "    prior_variance = np.diag(prior_var * np.ones((x_poly.shape[1], )))\n",
    "    prior_precision = np.linalg.inv(prior_variance)\n",
    "\n",
    "    #defining the posterior variance\n",
    "    joint_variance = np.linalg.inv(prior_precision + 1. / y_var * x_poly.T.dot(x_poly))\n",
    "    #defining the posterior mean\n",
    "    joint_mean = joint_variance.dot(x_poly.T.dot(y_matrix)) * 1. / y_var\n",
    "\n",
    "    #sampling S points from the posterior\n",
    "    posterior_samples = np.random.multivariate_normal(joint_mean.flatten(), joint_variance, size=S)\n",
    "    #sampling S points from the posterior predictive\n",
    "    y_predict_noiseless = np.array([x_test_poly.dot(sample) for sample in posterior_samples])\n",
    "    y_predict_bayes = y_predict_noiseless + np.random.normal(0, y_var**0.5, size=(S, len(x_test)))\n",
    "    \n",
    "    #compute log likelihood for the test data\n",
    "    log_likelihood_bayes = []\n",
    "    for n in range(len(y_test)):\n",
    "        log_likelihood_bayes.append(gaussian_log_pdf(y_predict_noiseless[:, n], y_var, y_test[n]).mean())\n",
    "    log_likelihood_bayes = np.array(log_likelihood_bayes)\n",
    "    \n",
    "    #compute the 95 percentiles of the posterior predictives\n",
    "    ub_bayes = np.percentile(y_predict_bayes, 97.5, axis=0)\n",
    "    lb_bayes = np.percentile(y_predict_bayes, 2.5, axis=0)\n",
    "    \n",
    "    #visualize the posterior predictive distribution\n",
    "    ax.scatter(x, y, color='red', s=10, alpha=0.5, label='train data')\n",
    "    ax.fill_between(x_test, ub_bayes, lb_bayes, color='blue', alpha=0.2, label=\"test log-likelihood: {}\".format(np.round(np.sum(log_likelihood_bayes), 4)))\n",
    "    ax.set_title('posterior predictive distribution of bayesian regression model with prior variance of {}'.format(prior_var))\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlim([-10, 10])\n",
    "    \n",
    "    return ax, joint_variance, joint_mean\n",
    "\n",
    "def black_box_variational_inference(logprob, D, num_samples):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implements http://arxiv.org/abs/1401.0118, and uses the\n",
    "    local reparameterization trick from http://arxiv.org/abs/1506.02557\n",
    "    code taken from:\n",
    "    https://github.com/HIPS/autograd/blob/master/examples/black_box_svi.py\n",
    "    \"\"\"\n",
    "\n",
    "    def unpack_params(params):\n",
    "        # Variational dist is a diagonal Gaussian.\n",
    "        mean, log_std = params[:D], params[D:]\n",
    "        return mean, log_std\n",
    "\n",
    "    def gaussian_entropy(log_std):\n",
    "        return 0.5 * D * (1.0 + np.log(2*np.pi)) + np.sum(log_std)\n",
    "\n",
    "    rs = npr.RandomState(0)\n",
    "    def variational_objective(params, t):\n",
    "        \"\"\"Provides a stochastic estimate of the variational lower bound.\"\"\"\n",
    "        mean, log_std = unpack_params(params)\n",
    "        samples = rs.randn(num_samples, D) * np.exp(log_std) + mean\n",
    "        lower_bound = gaussian_entropy(log_std) + np.mean(logprob(samples, t))\n",
    "        return -lower_bound\n",
    "\n",
    "    gradient = grad(variational_objective)\n",
    "\n",
    "    return variational_objective, gradient, unpack_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def variational_inference(Sigma_W, sigma_y, y_train, x_train, forward, S, max_iteration, step_size, verbose):\n",
    "    '''implements wrapper for variational inference via bbb for bayesian regression'''\n",
    "    D = Sigma_W.shape[0]\n",
    "    Sigma_W_inv = np.linalg.inv(Sigma_W)\n",
    "    Sigma_W_det = np.linalg.det(Sigma_W)\n",
    "    variational_dim = D\n",
    "    \n",
    "    #define the log prior on the model parameters\n",
    "    def log_prior(W):\n",
    "        constant_W = -0.5 * (D * np.log(2 * np.pi) + np.log(Sigma_W_det))\n",
    "        exponential_W = -0.5 * np.diag(np.dot(np.dot(W, Sigma_W_inv), W.T))\n",
    "        log_p_W = constant_W + exponential_W\n",
    "        return log_p_W\n",
    "\n",
    "    #define the log likelihood\n",
    "    def log_lklhd(W):\n",
    "        S = W.shape[0]\n",
    "        constant = (-np.log(sigma_y) - 0.5 * np.log(2 * np.pi)) * N\n",
    "        exponential = -0.5 * sigma_y**-2 * np.sum((y_train.reshape((1, 1, N)) - forward(W, x_train))**2, axis=2).flatten()\n",
    "        return constant + exponential\n",
    "\n",
    "    #define the log joint density\n",
    "    log_density = lambda w, t: log_lklhd(w) + log_prior(w)\n",
    "\n",
    "    #build variational objective.\n",
    "    objective, gradient, unpack_params = black_box_variational_inference(log_density, D, num_samples=S)\n",
    "\n",
    "    def callback(params, t, g):\n",
    "        if verbose:\n",
    "            if  t % 100 == 0:\n",
    "                print(\"Iteration {} lower bound {}; gradient mag: {}\".format(t, -objective(params, t), np.linalg.norm(gradient(params, t))))\n",
    "\n",
    "    print(\"Optimizing variational parameters...\")\n",
    "    #initialize variational parameters\n",
    "    init_mean = np.ones(D)\n",
    "    init_log_std = -100 * np.ones(D)\n",
    "    init_var_params = np.concatenate([init_mean, init_log_std])\n",
    "    \n",
    "    #perform gradient descent using adam (a type of gradient-based optimizer)\n",
    "    variational_params = adam(gradient, init_var_params, step_size=step_size, num_iters=max_iteration, callback=callback)\n",
    "    \n",
    "    return variational_params \n",
    "\n",
    "def variational_polynomial_regression(Sigma_W, sigma_y, x_test, y_test, x_train, y_train, forward, ax, posterior_sample_size=100, S=2000, max_iteration=2000, step_size=1e-2, verbose=True):\n",
    "    '''perform bayesian regression: infer posterior, visualize posterior predictive, compute log-likelihood'''\n",
    "    D = Sigma_W.shape[0]\n",
    "    \n",
    "    #approximate posterior with mean-field gaussian\n",
    "    variational_params = variational_inference(Sigma_W, sigma_y, y_train, x_train, forward, S, max_iteration, step_size, verbose)\n",
    "    \n",
    "    #sample from the variational posterior\n",
    "    var_means = variational_params[:D]\n",
    "    var_variance = np.diag(np.exp(variational_params[D:])**2)\n",
    "    posterior_samples = np.random.multivariate_normal(var_means, var_variance, size=posterior_sample_size)\n",
    "\n",
    "    #predict on x_test\n",
    "    y_predict_noiseless = forward(posterior_samples, x_test)\n",
    "    y_pred = y_predict_noiseless + np.random.normal(0, sigma_y**0.5, size=(posterior_sample_size, len(x_test)))\n",
    "\n",
    "    #Gaussian log pdf\n",
    "    gaussian_log_pdf = lambda mu, sigma_sq, x: -0.5 * (np.log(2 * np.pi * sigma_sq) + (x - mu)**2 / sigma_sq)\n",
    "\n",
    "    #compute log likelihood for the test data\n",
    "    log_likelihood = []\n",
    "    for n in range(len(y_test)):\n",
    "        log_likelihood.append(gaussian_log_pdf(y_predict_noiseless[:, n], sigma_y, y_test[n]).mean())\n",
    "    log_likelihood = np.array(log_likelihood).sum()\n",
    "\n",
    "    #compute the 95 percentiles of the posterior predictives\n",
    "    ub_bayes = np.percentile(y_pred, 97.5, axis=0)\n",
    "    lb_bayes = np.percentile(y_pred, 2.5, axis=0)\n",
    "    \n",
    "    #visualize the posterior predictive distribution\n",
    "    ax.scatter(x_train, y_train, color='red', s=10, alpha=0.5, label='train data')\n",
    "    ax.fill_between(x_test, ub_bayes, lb_bayes, color='green', alpha=0.2, label='test log-likelihood:{}'.format(np.round(log_likelihood, 4)))\n",
    "    ax.set_title('mean-field Gaussian variational approximation of the posterior')\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    return ax, var_variance, var_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Administrative Matters\n",
    "\n",
    "1. **Attendance Quiz:** https://tinyurl.com/y3qkvk5c<br><br>\n",
    "\n",
    "2. **Homework #7:** The suggested settings for running HMC on your BNN is so that you get a reasonable looking plot and a sense of how long HMC can take - your sampler will not have converged in 10,00 iterations!<br><br>\n",
    "\n",
    "3. **Homework #8:** The homework this week is not as intense as Homework #7. If you've made a good effort on Homework #7, then there isn't anything extra you need to implement - you just need to put pieces of code together.<br><br>\n",
    "\n",
    "4. **Project Check-point #2:** Reminder to contact your TF and instructor for a meeting if you haven't done so already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Review of Neural Network Models\n",
    "2. Bayesian Neural Networks\n",
    "3. Black-box Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review of Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Would You Parameterize a Non-linear Trend?\n",
    "In non-linear regression, we are interested in modeling observed outcome $Y^{(n)}$ as a non-linear function of observed covariates $\\mathbf{X}^{(n)}$:\n",
    "\\begin{align}\n",
    "\\mu &= g_{\\mathbf{w}}(\\mathbf{X}^{(n)})\\\\\n",
    "Y^{(n)}&\\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "\\end{align}\n",
    "But it's not easy to think of a function class $g(x)$ can capture the trend in the data (e.g. polynomial or sinusoical)?\n",
    "<img src=\"./fig/fig12.png\" style='height:400px;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing Arbitrarily Complex Functions\n",
    "\n",
    "**Motto:** neural netoworks build up a complex function $g$ by **composing** simple nonlinear functions. We represent neural networks as **layered directed graphs** where each node $i$ in the $l$-th layer represents the function $f\\left(\\sum_{j}w^{l-1}_{ji} h^{l-1}_j\\right)$, $h^{l-1}_j$ being the hidden nodes from the $l-1$-th layer.\n",
    "\n",
    "<img src=\"./fig/fig5.png\" style='height:300px;'>\n",
    "\n",
    "This is a ***neural network***. We denote the weights of the neural network collectively by $\\mathbf{W}$.\n",
    "The non-linear function $f$ is called the ***activation function***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Design Choices: Depth or Width\n",
    "Ideally, we want our architecture to be able to express a number of very complex functions, since we don't know what is appropriate for our data. So what architecture is more effective for expressing complex functions?\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./fig/wide.jpg\" style=\"height: 350px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/deep.jpg\" style=\"height: 350px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks Regression\n",
    "\n",
    "**Model for Regression:** $Y^{(n)}\\sim \\mathcal{N}(\\mu, \\sigma^2)$, $\\mu = g_\\mathbf{W}(\\mathbf{X}^{(n)})$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$.\n",
    "\n",
    "**Training Objective:** find $\\mathbf{W}$ to maximize the likelihood of our data. This is equivalent to minimizing the Mean Square Error,\n",
    "$$\n",
    "\\max_{\\mathbf{W}}\\, \\mathrm{MSE}(\\mathbf{W}) = \\frac{1}{N}\\sum^N_{n=1} \\left(y_n - g_\\mathbf{W}(x_n)\\right)^2\n",
    "$$\n",
    "\n",
    "**Optimizing the Training Objective:** The main challenge of optimizing the objective is computing the gradient of a neural network. Luckily, the gradient can be computed in an algorithmic way using the chain rule, working from the output node **backwards** towards the input. For example, the derivative with respect to the hidden node $h^l_i$ is:\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{MSE}}{\\partial h^l_i} = \\sum_{j}\\underbrace{\\frac{\\partial \\mathrm{MSE}}{\\partial h^{l+1}_j}}_{\\text{derivative from layer $l\\quad$}} \\underbrace{\\frac{\\partial h^{l+1}_j}{\\partial h^{l}_i}}_{\\text{derivative of $f\\left(\\sum_{k}w_k h_k\\right)$}}\n",
    "$$\n",
    "Using this backwards gradient computation, we can optimize a neural network with respect to the MSE using **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Maximum Likelihood Objective is Non-Convex for Neural Networks\n",
    "Unfortunately, the likelihood and MSE functions for neural network regression models are not convex! This means that **just because your gradient is zero, it doens't mean you've optimized anything**.\n",
    "\n",
    "<img src=\"./fig/loss_landscape.jpg\" style=\"height: 350px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Regression vs Linear Regression\n",
    "\n",
    "Linear models are easy to interpret. Once we've found the MLE of the model parameters, we can formulate scientific hypotheses about the relationship between the outcome $Y$ and the covariates $\\mathbf{X}$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{income}} = 2 * \\text{education (yr)} + 3.1 * \\text{married} - 1.5 * \\text{gaps in work history}\n",
    "\\end{align}\n",
    "\n",
    "What do the weights of a neural network tell you about the relationship between the covariates and the outcome?\n",
    "<img src=\"./fig/fig5.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization Error and Bias/Variance\n",
    "Complex models have ***low bias*** -- they can model a wide range of functions, given enough samples.\n",
    "\n",
    "But complex models like neural networks can use their 'extra' capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have ***high variance*** -- they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig11.png\" style=\"width: 380px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig12.png\" style=\"width: 380px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Polynomial Regression is Bayesian Linear Regression\n",
    "A Bayesian polynomial regression model uses a polynomial of degree $D$ to capture the relationship between $X\\in \\mathbb{R}$ and $Y$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{W} &\\sim \\mathcal{N}(0, \\sigma^2_W \\mathbf{I})\\\\\n",
    "\\mu^{(n)}&= w_0 + w_1 X^{(n)} + w_2 (X^{(n)})^2 + \\ldots + w_D(X^{(n)})^D\\\\\n",
    "Y^{(n)} &\\sim \\mathcal{N}(\\mu^{(n)}, \\sigma^2_Y)\n",
    "\\end{align}\n",
    "\n",
    "Rather than considering the polynomial as a non-linear function of $X$, we can see it as a linear function of the vector $\\mathbf{X} = [1, X, X^2, \\ldots, X^D] \\in \\mathbb{R}^{D+1}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{W} &\\sim \\mathcal{N}(0, \\sigma^2_W \\mathbf{I})\\\\\n",
    "\\mu^{(n)}&= \\mathbf{W}^\\top \\mathbf{X}^{(n)}\\\\\n",
    "Y^{(n)} &\\sim \\mathcal{N}(\\mu^{(n)}, \\sigma^2_Y)\n",
    "\\end{align}\n",
    "\n",
    "This means that for a Bayesian polynomial regression model, the posterior $p(\\mathbf{X} | \\text{Data})$ is a multivariate Gaussian (just as in the case of Bayesian linear regression, see HW#0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Neural Networks\n",
    "A **Bayesian neural network (BNN)** is a Bayesian model for regression that uses a neural network to capture the relationship between $\\mathbf{X}$ and $Y$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{W} &\\sim \\mathcal{N}(0, \\sigma^2_W \\mathbf{I})\\\\\n",
    "\\mu^{(n)}&= g_{\\mathbf{W}}(\\mathbf{X}^{(n)})\\\\\n",
    "Y^{(n)} &\\sim \\mathcal{N}(\\mu^{(n)}, \\sigma^2_Y)\n",
    "\\end{align}\n",
    "\n",
    "Unfortunately, the posterior of a neural network is multimodal and very complex, posing a challenge for samplers. Furthermore, training data for BNNs are typically large, this makes gradient-based samplers like HMC extremely inefficient -- in every leap-frog iteration of HMC, the gradient $\\frac{\\partial U(q)}{\\partial q}$ requires an evaluation over the entire training data set. \n",
    "\n",
    "**See course project papers:** \n",
    "1. NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport\n",
    "2. Stochastic Gradient Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Black-box Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review of Variational Inference\n",
    "\n",
    "**Goal:** given a target posterior distribution $p(\\psi | Y_1, \\ldots, Y_N)$, $\\psi \\in \\mathbb{R}^I$ we want to find a distribution $q(\\psi |\\lambda^*)$ in a family of distributions $Q = \\{q(\\psi |\\lambda) | \\lambda \\in \\Lambda \\}$ such that $q(\\psi |\\lambda^*)$ best approximates $p$. \n",
    "\n",
    "**Design Choices:** we need to choose:\n",
    "\n",
    "A. ***(Variational family)*** a family $Q$ of candidate distributions for approximating $p$. The members of $Q$ are called the ***variational distributions***.\n",
    "\n",
    "  **Our Choice:**  we assume that the joint $q(\\psi)$ factorizes completely over each dimension of $\\psi$, i.e. $q(\\psi)= \\prod_{i=1}^I q(\\psi_i | \\lambda_i)$. This is called the ***mean field assumption***. What can go wrong with this design choice?\n",
    "  \n",
    "B. ***(Divergence measure)*** a divergence measure to quantify the difference between $p$ and $q$.\n",
    "\n",
    "  **Our Choice:** \n",
    "  $$D_{\\text{KL}}(q(\\psi | \\lambda) \\| p(\\psi | Y_1, \\ldots, Y_N)) = \\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left( \\frac{q(\\psi | \\lambda)}{p(\\psi | Y_1, \\ldots, Y_N)} \\right) \\right]$$\n",
    "  What can go wrong with this design choice?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Inference as Optimization\n",
    "\n",
    "We can now formalize the the problem of variational inference for a target distribution $p(\\psi)$: \n",
    "\n",
    "Find a $q(\\psi|\\lambda^*)$ such that \n",
    "\n",
    "\\begin{aligned}\n",
    "\\lambda^* = \\underset{\\lambda}{\\text{argmin}}\\; D_{\\text{KL}}(q(\\psi|\\lambda) \\| p(\\psi|Y_1, \\ldots, Y_N))) = \\underset{\\lambda}{\\text{argmin}}\\; \\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{q(\\psi | \\lambda)}{p(\\psi|Y_1, \\ldots, Y_N))}\\right) \\right]\n",
    "\\end{aligned}\n",
    "\n",
    "Recall that for EM, we had proved that minimizing the KL is equivalent to maximizing the ELBO (for which it is easier to compute the gradient). We will do the same here:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\underset{\\lambda}{\\min}D_{\\text{KL}}(q(\\psi|\\lambda) \\| p(\\psi|Y_1, \\ldots, Y_N))) &\\overset{\\text{equiv}}{\\equiv} \\underset{\\lambda}{\\max} -D_{\\text{KL}}(q(\\psi|\\lambda) \\| p(\\psi|Y_1, \\ldots, Y_N))) \\\\\n",
    "&= \\underset{\\lambda}{\\max} -\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{q(\\psi | \\lambda)}{p(\\psi|Y_1, \\ldots, Y_N))} \\right)\\right] \\\\\n",
    "&= \\underset{\\lambda}{\\max}\\underbrace{\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{p(\\psi, Y_1, \\ldots, Y_N))}{q(\\psi | \\lambda)} \\right)\\right]}_{ELBO(\\lambda)} - \\log p(Y_1, \\ldots, Y_N).\n",
    "\\end{aligned}\n",
    "\n",
    "Thus, the variational objective can be rephrased as maximizing the $ELBO$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Inference for Bayesian Neural Networks\n",
    "\n",
    "Consider a Bayesian neural network:\n",
    "\\begin{align}\n",
    "\\mathbf{W} &\\sim \\mathcal{N}(0, \\sigma^2_W \\mathbf{I})\\\\\n",
    "Y^{(n)} &\\sim \\mathcal{N}(g_{\\mathbf{W}}(\\mathbf{X}^{(n)}), \\sigma^2_Y)\n",
    "\\end{align}\n",
    "and a mean-field Gaussian variational family:\n",
    "$$\n",
    "q(\\mathbf{W} | \\mu, \\Sigma) = \\mathcal{N}(\\mathbf{W}; \\mu, \\Sigma)\n",
    "$$\n",
    "where $\\Sigma$ is a diagonal matrix.\n",
    "\n",
    "The ELBO is: \n",
    "\\begin{align}\n",
    "ELBO(\\mathbf{W}) &= \\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)} \\left[ \\log \\left( \\frac{p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W})}{q(\\mathbf{W} | \\mu, \\Sigma)} \\right) \\right]\\\\\n",
    "&= \\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)} \\left[ \\log \\left( \\frac{\\mathcal{N}(\\mathbf{W}; 0, \\sigma^2_W \\mathbf{I}) \\prod_{n=1}^N \\mathcal{N}(Y^{(n)}; g_{\\mathbf{W}}(\\mathbf{X}^{(n)}), \\sigma^2_Y)}{\\mathcal{N}(\\mathbf{W}; \\mu, \\Sigma)} \\right) \\right].\n",
    "\\end{align}\n",
    "\n",
    "To find the optimal variational parameters such that $\\mu^*, \\Sigma^* = \\mathrm{argmax}\\, ELBO(\\mathbf{W})$, we need to compute the gradient of the ELBO:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\mu, \\Sigma}\\,\\underbrace{\\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)} \\left[ \\log \\left( \\frac{p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W})}{q(\\mathbf{W} | \\mu, \\Sigma)} \\right) \\right]}_{ELBO(\\mathbf{W})}.\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Black-Box Variational Inference\n",
    "\n",
    "The ***Black-box Variational Inference (BBVI)*** algorithm for BNN's:\n",
    "0. **Initialization:** pick an intial value $\\mu^{(0)}, \\Sigma^{(0)}$\n",
    "1. **Gradient Ascent:** repeat:\n",
    "\n",
    "   1. Approximate the gradient \n",
    "   \\begin{align}\n",
    "   \\nabla_{\\mu, \\Sigma} \\, ELBO(\\mathbf{W}) &= \\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)}\\left[ \\nabla_{\\mu, \\Sigma}\\, q(\\mathbf{W} | \\mu, \\Sigma) * \\log \\left( \\frac{p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W})}{q(\\mathbf{W} | \\mu, \\Sigma)} \\right) \\right]\\\\\n",
    "   &\\approx\\frac{1}{S}\\underbrace{\\sum_{s=1}^S \\nabla_{\\mu, \\Sigma}\\, \\log q(\\mathbf{W}^s | \\mu, \\Sigma) * \\log \\left( \\frac{p(\\mathbf{W}^s) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W}^s)}{q(\\mathbf{W}^s | \\mu, \\Sigma)} \\right)}_{{\\text{score function gradient}}},\n",
    "   \\end{align}\n",
    "   where $\\mathbf{W}^s\\sim q(\\mathbf{W} | \\mu^{\\text{current}}, \\Sigma^{\\text{current}})$.\n",
    "   2. Update parameters $(\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) \\leftarrow (\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) + \\eta * {\\text{score function gradient}}$\n",
    "   \n",
    "See Lecture #15 board-work or the appendix of \"Black-box Variational Inference\" for the derivation of the expression for the gradient of the ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance of the Gradient Estimate\n",
    "\n",
    "In Black-Box Variational Inference, we estimate the gradient using Monte Carlo estimation (i.e. the score function gradient approximation):\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mu, \\Sigma} \\, ELBO(\\mathbf{W}) \\approx \\frac{1}{S} {\\sum_{s=1}^S \\nabla_{\\mu, \\Sigma}\\,\\log q(\\mathbf{W}^s | \\mu, \\Sigma) * \\log \\left( \\frac{p(\\mathbf{W}^s) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W}^s)}{q(\\mathbf{W}^s | \\mu, \\Sigma)} \\right)},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^s\\sim q(\\mathbf{W} | \\mu^{\\text{current}}, \\Sigma^{\\text{current}})$.\n",
    "\n",
    "As in the case with every MC estimate, we worry about variance. As it turns out, the score function gradient approximation has very high variance. This leads to slow convergence for gradient descent. To mitigate this, we need to employ a number of **variance reduction methods**.\n",
    "\n",
    "In the original paper \"Black-Box Variational Inference\", the authors implement control variates and Rao-Blackwellization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient of the ELBO with the Reparametrization Trick\n",
    "\n",
    "An alternative to using the log-derivative trick to computing the gradient of the ELBO is to use the reparametrization trick. \n",
    "\n",
    "We note that since $q(\\mathbf{W} | \\mu, \\Sigma) = \\mathcal{N}(\\mathbf{W};\\mu, \\Sigma )$, sampling $W\\sim q(\\mathbf{W} | \\mu, \\Sigma)$ is equivalent to sampling $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$ and then transforming the sample $\\mathbf{W} = \\epsilon^\\top \\Sigma^{1/2} + \\mu$, where $\\mathbf{I}$ and $\\Sigma$ have the same dimensions.\n",
    "\n",
    "Thus, we can rewrite the ELBO:\n",
    "<img src=\"./fig/reparametrized_grad.png\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Black-Box Variational Inference with the Reparametrization Trick\n",
    "\n",
    "The ***Black-box Variational Inference (BBVI) with the reparametrization trick*** or ***Bayes By Backprop*** algorithm for BNN's:\n",
    "0. **Initialization:** pick an intial value $\\mu^{(0)}, \\Sigma^{(0)}$\n",
    "1. **Gradient Ascent:** repeat:\n",
    "\n",
    "   1. Approximate the gradient \n",
    "   \\begin{align}\n",
    "   \\nabla_{\\mu, \\Sigma} \\, ELBO(\\mathbf{W}) \\approx& \\frac{1}{S} \\sum_{s=1}^S \\nabla_{\\mu, \\Sigma} \\log \\left[p(\\epsilon_s^\\top \\Sigma + \\mu) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\epsilon_s^\\top \\Sigma + \\mu)\\right] \\\\\n",
    "   &- \\nabla_{\\mu, \\Sigma}\\underbrace{\\mathbb{E}_{\\mathbf{W} \\sim \\mathcal{N}(\\mu, \\Sigma )}\\left[\\log \\mathcal{N}(\\mathbf{W};\\mu, \\Sigma ) \\right]}_{\\text{Guassian entropy: has closed form}},\n",
    "   \\end{align}\n",
    "   where $\\epsilon_s \\sim \\mathcal{N}(0, \\mathbf{I})$.\n",
    "   2. Update parameters $(\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) \\leftarrow (\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) + \\eta * {\\text{score function gradient}}$\n",
    "   \n",
    "See Lecture #15 board-work or the appendix of \"Black-box Variational Inference\" for the derivation of the expression for the gradient of the ELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape (40,)\n",
      "y_train shape (40,)\n"
     ]
    }
   ],
   "source": [
    "##set random seed\n",
    "rand_state = 0\n",
    "random = np.random.RandomState(rand_state)\n",
    "\n",
    "##generate training data\n",
    "#number of points in each of the two segments of the domain\n",
    "N = 20\n",
    "#output variance\n",
    "y_var = 1.\n",
    "x_train, y_train, x_test, y_test = generate_data(N, y_var)\n",
    "print('x_train shape', x_train.shape)\n",
    "print('y_train shape', y_train.shape)\n",
    "\n",
    "##transform covariates for polynomial regression\n",
    "poly = PolynomialFeatures(1)\n",
    "#transform x: add polynomial features\n",
    "x_poly = poly.fit_transform(x_train.reshape((-1, 1)))\n",
    "#transform x_test: add polynomial features\n",
    "x_test_poly = poly.fit_transform(x_test.reshape((-1, 1)))\n",
    "\n",
    "##define dimensions\n",
    "N = x_poly.shape[0]\n",
    "D = x_poly.shape[1]\n",
    "\n",
    "##define variances\n",
    "sigma_y = 1.**2\n",
    "weight_noise = 5**2\n",
    "Sigma_W = weight_noise * np.eye(D)\n",
    "\n",
    "##polynomial function\n",
    "def forward(w, x):\n",
    "    x_poly = poly.fit_transform(x.reshape((-1, 1))) \n",
    "    return np.dot(w, x_poly.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BBVI for Bayesian Linear Regression (Posterior Predictives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing variational parameters...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bea6f29c2eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayesian_polynomial_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposterior_sample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoly_degree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariational_polynomial_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSigma_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_sample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposterior_sample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-35fd8d81bbb3>\u001b[0m in \u001b[0;36mvariational_polynomial_regression\u001b[0;34m(Sigma_W, sigma_y, x_test, y_test, x_train, y_train, forward, ax, posterior_sample_size, S, max_iteration, step_size, verbose)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#approximate posterior with mean-field gaussian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mvariational_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariational_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSigma_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#sample from the variational posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-35fd8d81bbb3>\u001b[0m in \u001b[0;36mvariational_inference\u001b[0;34m(Sigma_W, sigma_y, y_train, x_train, forward, S, max_iteration, step_size, verbose)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#perform gradient descent using adam (a type of gradient-based optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mvariational_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_var_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvariational_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(grad, x0, callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(grad, x, callback, num_iters, step_size, b1, b2, eps)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m      \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m  \u001b[0;31m# First  moment estimate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/misc/optimizers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, i)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_x0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0m_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n\u001b[1;32m     28\u001b[0m                         \"Try jacobian, elementwise_grad or holomorphic_grad.\")\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0munary_to_nary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/core.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/core.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(g, end_node)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoposort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mingrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moutgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_outgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \"VJP of {} wrt argnum 0 not defined\".format(fun.__name__))\n\u001b[1;32m     66\u001b[0m             \u001b[0mvjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjpfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0margnum_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnum_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/numpy/numpy_vjps.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munbroadcast_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0mtarget_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munbroadcast_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/numpy/numpy_vjps.py\u001b[0m in \u001b[0;36munbroadcast\u001b[0;34m(x, target_meta, broadcast_idx)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mtarget_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_iscomplex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtarget_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbroadcast_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2182\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "posterior_sample_size = 100\n",
    "##visualize the posterior predictive of a bayesian polynomial regression model\n",
    "#prior variance\n",
    "prior_var = 5**2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "ax[0], joint_variance, joint_mean = bayesian_polynomial_regression(x_train, y_train, x_test, y_test, prior_var, y_var, ax[0], S=posterior_sample_size, poly_degree=1)\n",
    "ax[1], var_variance, var_mean = variational_polynomial_regression(Sigma_W, sigma_y, x_test, y_test, x_train, y_train, forward, ax[1], posterior_sample_size=posterior_sample_size, S=4000, max_iteration=2000, step_size=1e-1, verbose=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BBVI for Bayesian Linear Regression (Posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#define points that include most of the probability mass of the pdf\n",
    "x, y = numpy.mgrid[-0.33:0.4:.005, 1.925:2.025:.005]\n",
    "pos = numpy.dstack((x, y))\n",
    "#get the value of the target pdf at those points\n",
    "z_p = sp.stats.multivariate_normal(joint_mean.flatten(), joint_variance).pdf(pos)\n",
    "#get the value of the variational pdf at those points\n",
    "z_q = sp.stats.multivariate_normal(var_mean, var_variance).pdf(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#plot the target density against variational densities\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.contourf(x, y, z_p, levels=14, cmap='Reds', alpha=0.8)\n",
    "ax.contour(x, y, z_p, levels=14, cmap='Reds', alpha=0.8)\n",
    "ax.contour(x, y, z_q, levels=14, cmap='Blues', alpha=1.)\n",
    "ax.set_title('Approximate posterior (blue) vs actual posterior (red)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
