{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #22: What is AM207?\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Review of the Course\n",
    "2. Beyond AM207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review of the Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Modeling Process\n",
    "\n",
    "<img src=\"fig/modeling.jpg\" style=\"height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What We Can Do So Far: Models\n",
    "1. **(Likelihood Models with Observed Variables)** When we have observed data $Y_{\\text{Obs}}$, we can model $Y_{\\text{Obs}}$ as a random variable $Y_{\\text{Obs}} \\sim p(Y |\\theta)$ with a known distribution $p$.\n",
    "  - if $Y_{\\text{Obs}}$ is a **label**, we can model it as a *Categorical* or *Bernoulli* variable \n",
    "  - if $Y_{\\text{Obs}}$ is a **count**, we can model it as a *Binomial*, *Multinomial* or *Poisson*\n",
    "  - if $Y_{\\text{Obs}}$ is **continuous**, we can model it as a *Gaussian*, *Exponential*, *Dirichlet* etc\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What We Can Do So Far: Models\n",
    "\n",
    "2. **(Likelihood Models with Latent Variables)**  When we also have unobserved data $Z_{\\text{Latent}}$, we can model $Z_{\\text{Latent}}$ and $Y_{\\text{Obs}}$ jointly $p(Y_{\\text{Obs}}, Z_{\\text{Latent}}| \\theta)$. \n",
    "\n",
    "  If $Y_{\\text{Obs}}$ is sequential, then the latent variable model is sequential, if we posit the Markov assumption for $Z_{\\text{Latent}}$, then we have a ***hidden markov model***.\n",
    "<br><br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Latent variable model$\\quad\\quad\\quad$\n",
    "        </td>\n",
    "        <td>\n",
    "            Hidden Markov Model$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"fig/latent_var.png\" style=\"height:150px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/hmm2.png\" style=\"height: 150px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What We Can Do So Far: Models\n",
    "\n",
    "3. **(Likelihood Models with Covariates)** When we believe that some observed variables $Y_{\\text{Obs}}$ depend on other observed variables $X_{\\text{Obs}}$, we can model $Y_{\\text{Obs}}$ as a random variable $Y_{\\text{Obs}} \\sim p(Y |X, \\theta)$ with a known distribution $p$. \n",
    "  - if $Y_{\\text{Obs}}$ is a **label**, we can model it as a *Categorical* or *Bernoulli* variable with parameters dependent on $X$, e.g. \n",
    "  $$Y_{\\text{Obs}} \\sim Ber(\\textrm{sigm}(f_\\theta (X))).$$\n",
    "  - if $Y_{\\text{Obs}}$ is a **count**, we can model it as a *Binomial*, *Multinomial* or *Poisson* variable with parameters dependent on $X$, e.g. \n",
    "  $$Y_{\\text{Obs}} \\sim Poi\\left(e^{f_\\theta(X)}\\right).$$\n",
    "  - if $Y_{\\text{Obs}}$ is **continuous**, we can model it as a *Gaussian*, *Exponential*, *Dirichlet* variable with parameters dependent on $X$, e.g. \n",
    "  $$Y_{\\text{Obs}} \\sim \\mathcal{N}(f_\\theta(X), \\sigma^2).$$\n",
    "  \n",
    "$\\quad$ The function $f$ is often a neural network with parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What We Can Do So Far: Models\n",
    "\n",
    "4. **(Bayesian Models)**  When we are being Bayesian, we *assume* a prior for $\\theta$, encoding our knowledge and uncertainty about $\\theta$. We model parameters and $Y_{\\text{Obs}}$ jointly:\n",
    "  - without covariates: \n",
    "  \\begin{align}\n",
    "  &p(Y_{\\text{Obs}}, \\theta) = p(Y_{\\text{Obs}} | \\theta)p(\\theta)\\\\\n",
    "  &p(Y_{\\text{Obs}}, Z_{\\text{Latent}}, \\theta) = p(Y_{\\text{Obs}}|Z_{\\text{Latent}}, \\theta) p(Z_{\\text{Latent}}|\\theta)p(\\theta).\n",
    "  \\end{align}<br><br>\n",
    "  - with covariates: \n",
    "  \\begin{align}\n",
    "  &p(Y_{\\text{Obs}}, \\theta | X_{\\text{Obs}}) = p(Y_{\\text{Obs}}| X_{\\text{Obs}}, \\theta)p(\\theta)\\\\\n",
    "  &p(Y_{\\text{Obs}}, Z_{\\text{Latent}}, \\theta | X_{\\text{Obs}}) = p(Y_{\\text{Obs}}| X_{\\text{Obs}}, Z_{\\text{Latent}}, \\theta)p(Z_{\\text{Latent}}|\\theta)p(\\theta)\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What We Can Do So Far: Inference\n",
    "We can make statements about $\\theta$ by performing:\n",
    "\n",
    "***Maximum Likelihood Estimation:*** for likelihood models, we compute a fixed value $\\theta_{\\text{MLE}}$ that **maximizes the likelihood** of the observed data $Y$.\n",
    "   \n",
    "***Bayesian Inference:*** for Bayesian models, we compute the ***posterior distribution*** $p(\\theta| Y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What We Can Do So Far: Inference Algorithm\n",
    "\n",
    "We choose an ***inference algorithm or method*** to perform inference:\n",
    "\n",
    "***Maximum Likelihood Estimation:*** <br>\n",
    "**A.** For *models with observed variables*, we:<br>\n",
    "$\\square$ analytically solve an unconstrained or contrained optimization problem to obtain $\\theta_{\\text{MLE}}$,<br>\n",
    "$\\square$ perform ***gradient descent*** to obtain $\\theta_{\\text{MLE}}$.\n",
    "    \n",
    "**B.** For *latent variable models*, we use ***expectation maximization*** to approximately find $\\theta_{\\text{MLE}}$.\n",
    "  \n",
    "***Bayesian Inference:*** <br>\n",
    "**A.** If the prior and likelihood are ***conjugate***, *analytically* derive the posterior distribution\n",
    "\n",
    "**B.** If the posterior distribution does not have a known form, sample from it using a ***sampler***.<br>\n",
    "$\\square$ Rejection sampling<br>\n",
    "$\\square$ MCMC sampling: Metropolis Hastings, Gibbs, Hamiltonian Monte Carlo<br>\n",
    "\n",
    "**C.** If the posterior distribution does not have a known form, approximate it using ***variational inference***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Happens After Inference?\n",
    "\n",
    "1. **(Predictive Evaluation)** In practice, we do not know the true model $\\theta_{\\text{True}}$! Thus, $\\theta_{\\text{MLE}}$ and $p(\\theta|Y)$ cannot be evaluated by comparison to $\\theta_{\\text{True}}$.\n",
    "  - ***Maximimum Likelihood Estimation:*** we compute $\\theta_{\\text{MLE}}$ on multiple bootstrap samples of the data; for each $\\theta_{\\text{MLE}}$ we sample $Y \\sim p(Y| \\theta_{\\text{MLE}})$. We compare these predictive samples with observed data $Y_{\\text{Obs}}$.\n",
    "  - ***Bayesian Inference:*** we sample $\\theta$'s from the posterior, for each $\\theta_{\\text{MLE}}$ we sample $Y \\sim p(Y| \\theta_{\\text{MLE}})$. We compare these ***posterior predictive samples*** with the observed data $Y_{\\text{Obs}}$.<br><br>\n",
    "  \n",
    "2. **(Uncertainty Evaluation)** Before making decisions with real-life consequence based on your model, you should check the precision of your estimate or uncertainty of you model.\n",
    "  - ***Maximimum Likelihood Estimation:*** repeat the MLE computation on many bootstrap samples of $Y_{\\text{Obs}}$. Compute the confidence interval of $\\theta$ and the predictive interval for $Y$. These intervals indicate *precision*.\n",
    "  - ***Bayesian Inference:*** Compute credible intervals for the posterior $p(\\theta|Y)$ and the predictive intervals of the posterior predictive. These intervals indicate *model uncertainty*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the Predictive Distributions\n",
    "\n",
    "How well does the predictive distribution of bootstrapped MLE models or the posterior predictive of a Bayesian model capture the actual observed data?\n",
    "\n",
    "We cannot, in general, perform visual chekcks (i.e. plot the predictive and the data). To quantify how well a predictive distribution aligns with the actual data we can compute:\n",
    "1. The KL-divergence between actual data (***empirical distirbution***) and our predictive distribution.\n",
    "2. The log-likelihood of the actual data. That is, we evaluate the observed data under the predictive distribution, i.e. the ***data log likelihood***  \n",
    "  $$\n",
    "  \\sum_{n=1}^N \\log  \\mathbb{E}_{p(\\theta|Y^{\\text{Obs}}_1, \\ldots, Y^{\\text{Obs}}_N)}\\left[ p(Y^{\\text{Obs}}_n | \\theta) \\right].\n",
    "  $$\n",
    "  \n",
    "In both cases the integrals we need to compute cannot be computed in close form and thus must be estimated using ***Monte Carlo methods***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating and Quantifying Uncertainty\n",
    "\n",
    "How do we know our model uncertainties (confidence intervals, credible intervals, predcitive intervals, posterior predictive intervals) are any good? What information about the data/model do we want our uncertainties to capture?\n",
    "\n",
    "**Epistemic Uncertainty:** uncertainty due to small number of samples across all scenarios. This can be reduced by more samples!\n",
    "\n",
    "**Aleatoric Uncertainty:** uncertainty due to the underlying randomness of the data generation process. This cannot be reduced no matter what.\n",
    "\n",
    "<img src=\"fig/epistemic.png\" style=\"height:250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond AM207 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The AIM of AM207\n",
    "\n",
    "**Lectures:** this course covers a broad selection of fundamental topics in statistical modeling. The goals are for any student who's worked hard in this course to be able to:\n",
    "1. build useful statistical models and perform meaningful analysis in their research domain\n",
    "2. continue in advanced coursework and/or research in probabilistic machine learning\n",
    "\n",
    "**Assignments & Projects:** the aim of the homework assignments and the project is to help you build technical proficiency to the point where you can engage with current research in this field, and so that picking up these skills again in the future will be much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning in the Wild\n",
    "\n",
    "Doing machine learning in real-life means extending the way we evaluate and critique models. We have to think about:\n",
    "\n",
    "**Covariate shift** - what happens if test data doesn't look like your train data?<br>\n",
    "$\\square$ validate your model on datasets from different time periods<br>\n",
    "$\\square$ validate your model on data collected from different sites<br>\n",
    "$\\square$ test your model adversarially (try to find edge cases)\n",
    " \n",
    "**Interpretability** - does your model generate meaningful scientific insight about the processes that generate data?<br>\n",
    "$\\square$ is your model structurally meaningful (e.g. in an hierarchical model)?<br>\n",
    "$\\square$ which (groups) of data feature(s) are most important for your model's predictions? Which training data points contributes the most to your model's accuracy?<br>\n",
    "$\\square$ can each decision of your model be approximated by a more 'interpretable' model?\n",
    " \n",
    "**Accountability & Safety** - your model is making decisions under uncertainty, are its decisions appropriately conservative given the consequences of being wrong in your specific application?<br>\n",
    "$\\square$ have you considered **how** your model will be used in the final decision system?<br>\n",
    "$\\square$ have you considered what laws apply to the outcomes effected by your model?<br>\n",
    "$\\square$ have you considered how to measure the impact of your model after deployment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Research at dtak\n",
    "**I. Sequential Decision Making (Reinforcement Learning)**<br>\n",
    "**II. Interpretability**<br>\n",
    "**III. Deep Bayesian Models**\n",
    "1. Improving inference for deep Bayesian/generative models: \n",
    "  - How can we capture better posterior predictives for BNNs?\n",
    "  - How can we learn better generative models for VAEs?\n",
    "2. Understanding properties of deep Bayesian/generative models:\n",
    "  - Can we design meaningful priors for BNNs?\n",
    "  - Can we prove properties about BNNs and VAEs? How do these properties inform evaluation and training procedures?\n",
    "3. Meaningful metrics for approximte inference:\n",
    "  - Can we meaningfully measure how well/poorly approximate posteriors capture the true posterior?\n",
    "  - Can we meaningfully measure the quality of the aleatoric and epistemic uncertainty provided by our model and inference procedure?\n",
    "4. Meaningful applications - are any of these fancy models actually useful in solving real tasks pertaining to:\n",
    " - diagnosing/prognosticating acute conditions using ICU data\n",
    " - diagnosing/prognosticating slow progressing conditions using ICU data\n",
    " - prognosticating probability of pregnancy/live birth using IVF data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
